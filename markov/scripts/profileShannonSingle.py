#!/usr/bin/env python3

"""Profiles the average and stadard deviation for Shannon entropy.

The average and standard deviation for the graph Shannon entropy is
calculated per linkograph size provided.

"""

import argparse # For command line parsing.
import numpy as np # For matrices.
import time # For getting the time to use as a random seed.
import math # For modf.
import json # For manipulating json files.
import matplotlib.pyplot as plt # For graphing.
import markov.Model as markel
import linkograph.stats as lstats

def genSingleOntologyStats(model, ontLink, minLinkoSize,
                           maxLinkoSize, stepLinkoSize,
                           runNum, precision=2, seeds=None):
    """Generate the stats on link models for a given ontology.

    inputs:

    model: the Markov model used to generate the linkographs.

    ontLink: ontology used for constructing linkographs.

    minLinkoSize: the minimun number of nodes in the linkographs to
    consider.

    maxLinkoSize: the maximum number of nodes in the linkographs to
    consider. Note that the max is not included to match pythons
    convertions on lists and ranges.

    stepLinkoSize: the step size between minLinkoSize to maxLinkoSize
    for the number of linkographs to Consider.

    runNum: the number of linkographs to consider for each linkograph
    size.

    precision:  the number of decimals places to use for the Markov
    models.

    output:

    a number_of _linkographs x 2. The (i, 0) entry provides the
    average Shannon entropy for the i-th size linkograph considered
    and the (i, 1) entry provides the standard deviation for of the
    Shannon entropy for the i-th size linkograph considered.

    """

    linkoSizes = range(minLinkoSize, maxLinkoSize, stepLinkoSize)

    results = np.zeros((len(linkoSizes), 2))

    # For each size linkograph, generate the runNum links and
    # caculate the needed statistics.
    for size in linkoSizes:

        print('size: {0}'.format(size))

        # Collect entropy.
        metrics = np.zeros(runNum)

        for i in range(runNum):

            # Randomize the initial state
            model.state = model.random.randint(1, len(model.absClasses)) - 1

            linko = model.genLinkograph(size, ontology=ontLink)

            entropy = lstats.graphEntropy(linko)

            metrics[i] = entropy

        # Find mean of the entropy
        index = (size - minLinkoSize) // stepLinkoSize
        results[index, 0] =  np.mean(metrics)
        results[index, 1] = np.std(metrics)

    return results


def genLinkMarkov(linkoSize, model, precision=2, timeSize=7):
    """Generates a link Markov from model generated linkograph.

    inputs:

    linkoSize: the size of linkograph to base the link Markov model
    off of.

    model: the Markov model to use. Note that the model must have an
    ontology in order to generate the linkgraphs.

    precicision: the number of decimal places to use for the
    link Markov model.

    timeSize = the size of integers to use for seeding the random
    number generator of the returned Markov model.

    output:

    A link Markov model based off a linkoSize linkograph generated by
    the provided Markov model.

    """

    seed = int(math.modf(time.time())[0]*(10**timeSize))

    # generate the linkograph
    linko = model.genLinkograph(linkoSize)

    # create the link model
    model = genModelFromLinko(linko, precision=precision,
                              ontology=model.ontology, seed=seed,
                              method='link', linkNum=1)
    
    return model

if __name__ == '__main__':

    info = """Investigates the graph Shannon Entropy for random Markov
    models."""

    parser = argparse.ArgumentParser(description=info)
    parser.add_argument('model', metavar='MODEL.json',
                        help='Markov model for profiling.')

    parser.add_argument('ontLink', metavar='ONTOLOGY_LINK.json',
                        help='the ontology file for learning.')

    parser.add_argument('-m', '--minimum', type=int, default = 2,
                       help='minimum size of linkographs.')

    parser.add_argument('-M', '--maximum', type=int, default = 100,
                       help='maximum size of linkographs.')

    parser.add_argument('-s', '--step', type=int, default = 1,
                       help='step size of linkographs.')

    parser.add_argument('-r', '--runs', type=int, default = 100,
                        help='the number of runs.')

    parser.add_argument('-p', '--precision', type=int, default = 2,
                        help='the number of runs.')

    args = parser.parse_args()

    # Extract the ontology
    model = markel.readJson(args.model)

    ontLink = None
    with open(args.ontLink, 'r') as ontLinkFile:
        ontLink = json.load(ontLinkFile)

    results = genSingleOntologyStats(model=model,
                                     ontLink=ontLink,
                                     minLinkoSize=args.minimum,
                                     maxLinkoSize=args.maximum,
                                     stepLinkoSize=args.step,
                                     runNum=args.runs,
                                     precision=args.precision)

    absClasses = list(model.absClasses)

    linkoSizes = range(args.minimum, args.maximum, args.step)

    plt.figure(1)

    plt.subplot(211)
    plt.plot(linkoSizes, results[:, 0])

    plt.xlabel('Size of Linkograph')
    plt.ylabel('Mean Shannon Entropy')
    plt.title('Mean Shannon Entropy per Linkograph Size')

    plt.subplot(212)
    plt.plot(linkoSizes, results[:, 1])

    plt.xlabel('Size of Linkograph')
    plt.ylabel('Standard Deviation Shannon Entropy')
    plt.title('Standard Deviation for Shannon Entropy'
              ' per Linkograph Size')

    plt.tight_layout()

    plt.show()
